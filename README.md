# Pebble
An AI-powered Mobile App to Supplement Children with Autism.

## Inspiration
Many people around the world are on a different playing field when it comes to learning because of the mental disabilities they were born with. Our goal with Pebble is to even out the playing field for these people. As of 2020, _1 in 36_ children were diagnosed with autism, and society does not have the infrastructure to adequately support their learning. They learn different and understand different, so we built Pebble to foster a better academic community that can support their learning. 

Now, with Pebble, they have access to the quality education that others have, in their own style.

## What It Does
Pebble is a mobile app (with connection to a backend) that provides AI-generated video lessons and fill-in-the-blank style questions to check for student understanding. 

The content for the AI-generated video lessons are extracted from standard class lectures that a normal student may attend. We devised a system to take the key concepts from standard lectures and present them in a format appealing and concise for our children with autism. Teachers can upload their recorded class lectures, and from there, each lesson is analyzed and dynamically added to a database where a student can access the new lesson from the app. 

The fill-in-the-blank questions are also generated by our system. We take the important sentences from our lessons and generate a question from that sentence.

The mobile app presents all the lessons stored by the database in an easy-to-understand manner for the student. They can select a lesson and watch the video with help from a transcript if needed. From there, they can answer questions and receive instant feedback from our system about their understanding.

## How We Built It
Pebble was in JavaScript and Python; JS was used in React Native to develop the mobile app, and Python was used in the AI system and backend.

### React Native
We chose React Native because we had a team member who was comfortable working with JavaScript. React Native is an open-source mobile application framework developed by Facebook. It offers cross-platform compatibility, enabling developers to build apps for both iOS and Android using a single codebase. React Native provides extensive library support, hot reloading for faster development, and benefits from an active community with comprehensive documentation.

#### Libraries
First, we had to add the necessary libraries and modules to make our app run smoothly. Some of the more important ones that we installed from npm included the Linear Gradient library, Video Player library, and the Navigation Container. Additionally, we used several components provided by React Native itself, as expected. Some of the more abstract components we used include Touchable Opacity, Stylesheet, and Dimensions.

#### Helper Components
To maintain efficiency and consistency in the designs across our screens, we leveraged one of React's most prominent features: reusable components. We created components such as Header and NextButton that could be used across multiple screens, while still allowing customization through the use of props. As a result, you may observe variations in headers or buttons due to the ability to modify their appearance while preserving their basic blueprint.

#### Hooks & State
A significant aspect of React is its hooks, which allow us to alter the state of a component. In this project, we employed hooks to create more dynamic visuals by changing the state, consequently showing or hiding certain elements through conditionals and operations. The two most notable examples of state changes are the search bar and the next button. On the home page, modifying the input text at the top triggers a call to a function that updates the text's state (displayed as a string value). This state can then be compared with the titles of the videos below to determine which one gets displayed. Similarly, the next button displays whatever value it holds in its state, allowing the "onPress" function to use that text to determine its functionality when clicked. Furthermore, we have employed state in innovative ways, leveraging its functionality to determine the background color of answer choices. The color scheme includes white, green, and red, representing whether an option is selected, correct, or incorrect, respectively.

#### Data Transfer & Framework Implementation
The final step of this project involved combining the JSON file provided by the Python portion of the project with the React Native side. To achieve this, we utilized props with the navigation property to transfer information across different pages. This process required understanding new concepts, including the route keyword when obtaining data through navigation props. Additionally, to avoid confusion in React Native, each framework in the JSON file was associated with its own component using the built-in map function, and we ensured each component had a unique identifier.

### Python
We chose Python for the AI system and backend because of it's ease of use and our comfort level. Two of our members were experienced in developing machine learning algorithms and web applications with Python. We created a module, also named Pebble, that would serve as our system for converting class lectures to Pebble lessons.  We opted for a module because it would let us organize the many functions we needed letting us access whatever we need wherever.

#### Transcription
Inside this module, we created functions that would transcribe an .mp4 file into text accounting for volume issues and pauses. We used Python libraries such as PyDub, Speech Recognition, and Deep Multilingual Punctuation to achieve this. We used PyDub to convert uploaded .mp4 files into .wav files that we could feed into Speech Recognition. Speech Recognition let us send our wav file to Google and receive a transcribed version that was adjusted for any potential issues. The Google transcript didn't return with any punctuation, so we used another natural language processing package to add punctuation.

#### Summarization & Script Generation
That transcript would then be fed into a summarization function (which uses the TF-IDF method). We used this method because it would let us measure the average value of a sentence and compare it to a threshold that would determine whether we keep that sentence. This summary was used for the script. The transcript would also be fed into an alternative summarization method (which uses BERT and the "xlnet-base-cased" model) that condenses the transcript much more than the other method. We had to use these two methods due to issues and problems we ran into covered in that [section](#challenges-we-ran-into)

#### Question Generation
We fed the summary generated by the BERT summarization method into a function that generated the fill-in-the-blank questions. For each sentence in this summary, we used natural language processing to extract the main keyword of the sentence. This was done using the SUMMA and RAKE_NLTK packages. The keywords we generated were then removed from the sentence and replaced with a blank. That keyword was shuffled into a list with random words (from MIT) and that list was returned as the answer choices alongside the correct answer's index.

#### Framework Generation
The transcript, the summary, and the questions were then fed into a function responsible for generating a framework that videos would then be generated from. This framework was necessary because it was also sent to the frontend mobile app for information about videos. This function split the summary into sentences, and for each sentence, found an image on the internet that best matches the overall meaning (through use of the keyword functions discussed above and Bing). It combined all this information into a JSON element that can be used by the front end and by the backend server.

#### Video Generation
Since for each sentence an image representing the sentence was found, the effective strategy would be to generate small clips for each sentence them combine them. We wrote a function that accomplishes that task through use of FFMPEG, gTTS, requests, and MoviePy. The function generates audio through text-to-speech, puts that on a white background in a video, then places the found image on top. It does this for each sentence and image, then combines them together. Due to issues that we'll cover in [challenges](#challenges-we-ran-into), we also uploaded this video to a CDN (for current purposes, that means YouTube).

#### Backend
The backend is a simple Flask application acting as an API. The frontend makes API calls to the backend to receive information about the lessons to display them. The backend also has a route where teachers can upload content directly to our platform. The content is, of course, analyzed with the functions explained above.

## Challenges We Ran Into
Throughout development of Pebble, we encountered several challenges that forced us to think outside the box to complete the task.

### Design

### React Native
Making UX and UI that worked well for the users and the backend led to several challenges:
1. **Libraries:** Although React Native had many packages that assisted us during the development phase, we encountered issues with outdated packages that required extensive support and research to get them working properly. Often, multiple packages offered the same functionality in different ways, and we had to choose the most efficient and accurate ones to meet our demands. Communication errors between React Native, Expo, and their dependencies resulted in multiple crashes, slow Android emulators, and confusion due to missing documentation. Despite these hurdles, we managed to find and integrate libraries that worked, allowing us to keep track of them for future projects.
2. **Helper Components:** Designing the Helper Components presented a challenge as they needed to serve as a usable blueprint while being flexible enough to adapt to various contexts where they were used. The NextButton component required significant tweaking. We maintained consistent styling throughout the pages, while the text was customizable, allowing it to change based on user input during the questions section. Implementing navigation through props posed another challenge, as the navigation prop is passed by default to pages but not to helper components. To resolve this, we employed property calls back and forth between parent and child components to achieve the desired results.
3. **Hooks & State:** Working with React Hooks to change state proved more complex than expected. There are different ways to handle state changes in React Native, and I had to research and understand the nuances. The standard method was used for the search bar, updating the typed text value. However, changing between states that involved true and false values, like when coding the transcript button, didn't work as expected. I discovered that React combines multiple button presses into one, making it challenging to intuitively figure out a new state based on a previous value. To address this, I modified my code to use an arrow function within the setState function to force React to base the current state on the previous one. Additionally, executing code directly after a state change required the use of a callback function to ensure React doesn't skip over the call.
4. **Data Transfer & Framework Implementation:** This was the most challenging part of the project on the React Native side. To efficiently manage and send data from our backend, we needed to use both React and default JavaScript. Challenges included correct mapping, deciphering the format of the JSON file, ensuring proper unique IDs, and dealing with long value names during use. As our JSON file contained multiple individual frameworks, we required a new component for each framework to map correctly. The JSON file format was initially confusing, necessitating proper formatting on the React Native side. After addressing this, we encountered a warning from React Native about unique IDs, which we resolved by adding an ID property to each framework. Finally, when using the data at the end stage, we faced issues with long and unreadable names. To improve readability, we learned how to destructure the data and filter them out as individual variables. For example, 'this.props.route.params.answers' became 'answers,' which is much more readable.

### Python
In the video generation, we encountered several problems:
1. **Transcription:** With transcribing videos, we were unaware that we could only send a maximum of 10MB to Google at a time. We had to recreate our transcription function to split audio into chunks that would be sent one at a time. Then, the program needed to combine the returned transcription from each of them. We couldn't split the audio when people were speaking, so we had to check for the volume level to find moments of silence that we could split. Google also did not return a transcription with punctuation, which we suspect to be because of the chunk system, so we had to use another package to add punctuation dynamically.
2. **Summarization:** When summarizing our transcript, we initially wanted to use TF-IDF to summarize. We developed the algorithm, and put it to test. The threshold we were using was not high enough, so lots of unimportant information was being included. We had to increase the threshold to a point where the video would get enough content, but the information would be accurate. This was a major issue because we struggled to decide whether we wanted more content in our videos or more accuracy. The BERT Summarization method detailed above also initially emerged for this decision. The BERT summarization was accurate, but only returned about 2-3 sentences of content. We decided that we needed more, so we reserved the BERT summarization for question generation, and we used the TF-IDF method for script summarization.
3. **Questions:** Generating questions was probably where we had the most challenges with. Initially, we sought to generate multiple choice questions instead of fill in the blank. We thought that those would be much better for understanding and retention. However, due to time constraints, we didn't want to train our own question generation model. We tried finding packages online that, combined with the top models from Hugging Face, would generate high quality questions. We found a package that would suit our needs, but we couldn't find a model that would work well. We found models that were either too slow or didn't return quality questions. Because of this, we opted for the fill in the blank question type. This question type was much easier to implement, as it didn't require any external packages or models. We would be using the same keyword recognition code detailed above to make the questions.
4. **Video Generation:** Video Generation and serving the video presented another problem for us. The optimal system for us would include buffering videos stored on our backend, but we found issues with video players on the front end making it hard to implement this. We opted for a CDN (through YouTube) to store our videos and programatically uploaded the generated videos to YouTube. Since we had initially wanted to use a system with buffering, the videos we generated were split up into segments and weren't combined. We had to make the last minute change due to the video player package not working correctly to uploading videos online and serving from there.

## Accomplishments That We're Proud Of

## What We Learned
In a more abstract sense, we learned that developing software or basic programs is a battle of tradeoffs—what you build won't be perfect, but you can optimize it for the resources that you have.

In a more technical sense, we learned to integrate React Native applications with a backend written in Python. We learned about integrating several modules together, resolving pip conflicting package version issues, and collaborating through Git. We learned how to effectively navigate the issues we faced and how to prevent them in the future.

## Experience
The three members of this team have been friends since elementary school. During the pandemic, we moved thousands of miles apart from each other and switched schools. We couldn't meet each other every day, go and hang out, and collaborate on projects anymore. The TechXCelerate Hackathon gave us the opportunity to work together again after several years. We made lots of memories during our two weeks working on this project, and we're grateful to have participated. We hope that Pebble can be taken another step further and truly help children with autism.

Thank you.
